{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7469e76",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "Decision trees model belongs to Supervised learning technique to solve either Regression or Classification type problems. As the name suggest, the process here is to narrow down the decision making process by creating branches and ultimately leaves to classify/predict output variable. The branch nodes represent the dataset features and leaves represent the outcomes. The model simply asks a question of Yes/No starting from the root node and depending on the answer it will either create a new branch or add to the existing branch. The process continues till the decision is reached and leaf nodes are created as outcomes, thus creating a tree like structure in a graphical representation. One thing to note here is, the more the number of predictor variables the more branches and bigger a tree.\n",
    "\n",
    "Consider an app recommender program/model. Assume we have a data of gender, occupation of few users and what apps they use.\n",
    "We can classify this based on either gender or occupation and recommend appropriate app. But is this accurate? Well, it's never accurate but it must be close to accurate. If gender and occupation are both combined as predictor variables to recommend the app we can get better results. \n",
    "\n",
    "\n",
    "![App](App.png)\n",
    "\n",
    "\n",
    "Another example is of student admissions based on their test scores and grades. Below we can depict that the data is well classified by a vertical line at test score 5 and further classification through grades will provide more accurate outcomes of whether a student should be accepted or rejected based on two variables mentioned above.\n",
    "\n",
    "![add](add.png)\n",
    "\n",
    "\n",
    "## Entropy:\n",
    "\n",
    "In physics, entropy is defined as the amount of randomness, or amount of freedom the atoms have to move in its current state. So any gas has the highest entropy in all states of matter. This can be co-related with the amount of probability in statistics. The more the probability of an event to occur the more entropy of that state.\n",
    "\n",
    "Entropy formula is given in terms of average of logarithm addition of the probabilites of events to occur for a given situation. This might be confusing to understand at first. Now, consider an example of probablity of winning in a game where we have to pick the same combination of balls available in the bucket. See below:\n",
    "\n",
    "![ballGame](ballGame.png)\n",
    "\n",
    "\n",
    "Here, the probabality of winning a game in each bucket is the product of the all the probabilities of picking a ball of each colour out of total number of balls. \n",
    "\n",
    "![log1](log1.png)\n",
    "\n",
    "The log values returned are negative, hence negative of log is taken so that we the get entropy to be positive. This can be generalized by a formula given below where m and n are number of variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb66f8c",
   "metadata": {},
   "source": [
    "$Entropy = -\\left(\\frac{m}{m+n}\\right) log_2\\left(\\frac{m}{m+n}\\right) - \\left(\\frac{n}{m+n}\\right) log_2\\left(\\frac{n}{m+n}\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88e87ff",
   "metadata": {},
   "source": [
    "![formula](formula.png)\n",
    "\n",
    "\n",
    "### Problem Statement 1:\n",
    "\n",
    "What is the entropy for a bucket with a ratio of four red balls to ten blue balls? Input your answer to at least three decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22284b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy value is 0.863\n"
     ]
    }
   ],
   "source": [
    "import math # This will import math module\n",
    "# assign values of m and n\n",
    "m, n = 4, 10\n",
    "\n",
    "entropy = -(m/(m+n)) *math.log2(m/(m+n)) -(n/(m+n)) *math.log2(n/(m+n)) \n",
    "print(\"Entropy value is {:0.3f}\".format(entropy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405d7769",
   "metadata": {},
   "source": [
    "Above entropy equation can be more generalized for multi class case as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d493bd",
   "metadata": {},
   "source": [
    "\n",
    "$Entropy = -p_1 log_2(p_n) - p_2 log_2(p_2) - ....  - \\sum \\limits _{i=1} ^{n}p_n log_2(p_n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a36289",
   "metadata": {},
   "source": [
    "### Problem Statement 2:\n",
    "\n",
    "If we have a bucket with eight red balls, three blue balls, and two yellow balls, what is the entropy of the set of balls? Input your answer to at least three decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5eeec928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy value is 1.335\n"
     ]
    }
   ],
   "source": [
    "# create a list of items\n",
    "li = [8,3,2]\n",
    "entropy=0\n",
    "for i in range(len(li)):\n",
    "    entropy = entropy -(li[i]/sum(li) *math.log2(li[i]/sum(li)))\n",
    "    \n",
    "print(\"Entropy value is {:0.3f}\".format(entropy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c52a9d",
   "metadata": {},
   "source": [
    "### Information Gain:\n",
    "\n",
    "Information gain is the amount of change in entropy. Well, initially the model is unorganized, when we perform different algorithms or make a decision tree the entropy of each node will keep changing. So the information gain will be the difference between entropy of parent and average entropy of its child. Information gain ranges from 0 to 1. Information gain 1 means the tree is having the best split of data while 0 means the worst split of data.\n",
    "\n",
    "![info](info.png)\n",
    "\n",
    "The idea is to generate or increase the information gain in a decision tree ultimately giving us the more justified split. Let's consider an earlier example of app recommendation to prove this.\n",
    "\n",
    "![MaxInfo](MaxInfo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e69f1f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent Entropy value is 1.46\n",
      "Child Entropy value of Female is 0.92\n",
      "Child Entropy value of Male is 0.92\n",
      "Information gain is : 0.54 \n",
      "\n",
      "Our aim is to get closer to information gain of 1 so will split the data now with occupation\n",
      "\n",
      "Parent Entropy value is 1.46\n",
      "Child Entropy value of Study is 0.00\n",
      "Child Entropy value of Work is 0.92\n",
      "Information gain is : 1.00 \n",
      "\n",
      "So we can see that splitting data with occupation we get information gain of 1 which is what we would like as\n",
      "it splits the data perfectly.\n"
     ]
    }
   ],
   "source": [
    "# we will make a fun that will calculate entropy for us\n",
    "def entropyCalc(li):\n",
    "    entropy=0\n",
    "    for i in range(len(li)):\n",
    "        entropy = entropy -(li[i]/sum(li) *math.log2(li[i]/sum(li)))\n",
    "    return round(entropy,2)\n",
    "\n",
    "# Supply a list of items, 3 for Pokeom, 2 for whatsapp and 1 for snapchat\n",
    "\n",
    "entropy_parent = entropyCalc([3,2,1])   \n",
    "print(\"Parent Entropy value is {:0.2f}\".format(entropy_parent))\n",
    "\n",
    "#now we will split the data by gender, that is F and M. \n",
    "#child entropy for Female \n",
    "#we will have 2 whatspp and 1 pokemon among female users\n",
    "\n",
    "entropy_child_F = entropyCalc([2,1])  \n",
    "print(\"Child Entropy value of Female is {:0.2f}\".format(entropy_child_F))\n",
    "\n",
    "#Similary child entropy for Male\n",
    "#we will have 2 Pokemon and 1 Whatsapp among male users\n",
    "\n",
    "entropy_child_M = entropyCalc([2,1]) \n",
    "print(\"Child Entropy value of Male is {:0.2f}\".format(entropy_child_M))\n",
    "print(\"Information gain is : {:0.2f} \\n\".format(entropy_parent-(entropy_child_F+entropy_child_M)/2)) \n",
    "\n",
    "print(\"Our aim is to get closer to information gain of 1 so will split the data now with occupation\\n\")\n",
    "\n",
    "# In occupation we have work and study as classifiers\n",
    "# for Study we only have Pokemon with 3 users, hence\n",
    "print(\"Parent Entropy value is {:0.2f}\".format(entropy_parent))\n",
    "entropy_child_S = entropyCalc([3]) \n",
    "print(\"Child Entropy value of Study is {:0.2f}\".format(entropy_child_S))\n",
    "\n",
    "# for Study we only have Pokemon with 2 whatsapp and 1 snapchat users, hence\n",
    "\n",
    "entropy_child_W = entropyCalc([2,1]) \n",
    "print(\"Child Entropy value of Work is {:0.2f}\".format(entropy_child_W))\n",
    "print(\"Information gain is : {:0.2f} \\n\".format(entropy_parent-(entropy_child_S+entropy_child_W)/2)) \n",
    "\n",
    "print(\"So we can see that splitting data with occupation we get information gain of 1 which is what we would like as\\nit splits the data perfectly.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf9cf28",
   "metadata": {},
   "source": [
    "However, we can still classify this further based on gender and get better results. So use information gain to determine which variable can be use to split the data and step by step develop the decision tree model.\n",
    "\n",
    "![finalGain](finalGain.png)\n",
    "\n",
    "### Problem Statement 3:\n",
    "\n",
    "For the following quiz, consider the data found in this file, consisting of twenty-four made-up insects measured on their length and color.\n",
    "\n",
    "Which of the following splitting criteria provides the most information gain for discriminating Mobugs from Lobugs?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62014f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating parent entropy of Species first which is our classification variable\n",
      "\n",
      "Parent Entropy value is 0.98000\n",
      "Child Entropy value is 1.00000 and 0.74000\n",
      "Information gain is : 0.11000 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(r'C:\\Everything On This PC\\Udacity\\Intro to ML -TensorFlow\\Decision Trees\\ml-bugs.csv')\n",
    "# get data of colors from dataframe\n",
    "brown,blue,green =0,0,0\n",
    "for data in df['Color']:\n",
    "    if data=='Brown':\n",
    "        brown+=1\n",
    "    elif data=='Blue':\n",
    "        blue+=1\n",
    "    else:\n",
    "        green+=1\n",
    "        \n",
    "#get the amount of Mobug and Lobug from dataframe\n",
    "mobug, lobug =0,0\n",
    "for data in df['Species']:\n",
    "    if data == 'Mobug':\n",
    "        mobug+=1\n",
    "    else:\n",
    "        lobug+=1 \n",
    "mobug_L1,mobug_L2=0,0\n",
    "lobug_L1,lobug_L2=0,0\n",
    "for i in range(len(df['Color'])):\n",
    "    if df['Length (mm)'][i] <17 and df['Species'][i] == \"Mobug\":\n",
    "        mobug_L1 +=1\n",
    "    elif df['Length (mm)'][i] >17 and df['Species'][i] == \"Mobug\":\n",
    "        mobug_L2 +=1\n",
    "    elif df['Length (mm)'][i] <17 and df['Species'][i] == \"Lobug\":\n",
    "        mobug_L1 +=1\n",
    "    elif df['Length (mm)'][i] >17 and df['Species'][i] == \"Lobug\":\n",
    "        lobug_L2 +=1\n",
    "\n",
    "print(\"Calculating parent entropy of Species first which is our classification variable\\n\")\n",
    "entropy_parent = entropyCalc([mobug,lobug])\n",
    "entropy_child_L1 = entropyCalc([mobug_L1,lobug_L1+mobug_L1])\n",
    "entropy_child_L2 = entropyCalc([mobug_L2,lobug_L2+mobug_L2])\n",
    "print(\"Parent Entropy value is {:0.5f}\\nChild Entropy value is {:0.5f} and {:0.5f}\".format(entropy_parent,entropy_child_L1,entropy_child_L2 ))\n",
    "\n",
    "print(\"Information gain is : {:0.5f} \\n\".format(entropy_parent-(entropy_child_L1+entropy_child_L2)/2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627220e4",
   "metadata": {},
   "source": [
    "## Hyperparameters for Decision Tree:\n",
    "\n",
    "There are few parameters in Decision Tree modeling which will help make the model generalize to different problems and useful for optimized classification. \n",
    "\n",
    "### Maximum Depth\n",
    "Depth is the length of each branch and the largest possible length in a tree is the maximum depth. For a depth of k there will be $2^k$ leaves.\n",
    "\n",
    "![maxDepth](maxDepth.png)\n",
    "\n",
    "### Minimum Number of Samples to Split\n",
    "There must be enough samples in the last node to split it into a new branch. Consider a case where minimum number of samples to split is 11.\n",
    "\n",
    "![split](split.png)\n",
    "\n",
    "\n",
    "### Minimum Number of samples per Leaf\n",
    "\n",
    "There could be scenarios where the last split might have samples, say 25, while the other node having 2 samples. In this case we cannot further carry the splitting with the node having 2 samples and it will not result in good classification. With minimum number of samples per leaf, we can make sure that each node will have enough samples to make sense of data. The aim is to not have extremely large difference in samples of two leafs.  \n",
    "\n",
    "![leaf](leaf.png)\n",
    "\n",
    "### Problem Statement 4:\n",
    "\n",
    "In this quiz, you'll be given the following sample dataset, and your goal is to define a model that gives 100% accuracy on it.\n",
    "\n",
    "The data file can be found under the \"data.csv\" tab in the quiz below. It includes three columns, the first 2 comprising of the coordinates of the points, and the third one of the label.\n",
    "\n",
    "The data will be loaded for you, and split into features X and labels y.\n",
    "You'll need to complete each of the following steps:\n",
    "\n",
    "**1. Build a decision tree model**\n",
    "\n",
    "Create a decision tree classification model using scikit-learn's DecisionTreeClassifier and assign it to the variablemodel.\n",
    "\n",
    "**2. Fit the model to the data**\n",
    "\n",
    "You won't need to specify any of the hyperparameters, since the default ones will yield a model that perfectly classifies the training data. However, we encourage you to play with hyperparameters such as max_depth and min_samples_leaf to try to find the simplest possible model.\n",
    "\n",
    "**3. Predict using the model**\n",
    "\n",
    "Predict the labels for the training set, and assign this list to the variable y_pred.\n",
    "\n",
    "**4. Calculate the accuracy of the model**\n",
    "\n",
    "For this, use the function sklearn function accuracy_score. A model's accuracy is the fraction of all data points that it correctly classified.\n",
    "\n",
    "When you hit Test Run, you'll be able to see the boundary region of your model, which will help you tune the correct parameters, in case you need them.\n",
    "\n",
    "Note: This quiz requires you to find an accuracy of 100% on the training set. This is like memorizing the training data! A model designed to have 100% accuracy on training data is unlikely to generalize well to new data. If you pick very large values for your parameters, the model will fit the training set very well, but may not generalize well. Try to find the smallest possible parameters that do the job—then the model will be more likely to generalize well. (This aspect of the exercise won't be graded.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1be9f2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is : 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import statements \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Read the data.\n",
    "data = np.asarray(pd.read_csv(r'C:\\Everything On This PC\\Udacity\\Intro to ML -TensorFlow\\Decision Trees\\data.csv', header=None))\n",
    "# Assign the features to the variable X, and the labels to the variable y. \n",
    "X = data[:,:2]\n",
    "y = data[:,2]\n",
    "\n",
    "\n",
    "# TODO: Create the decision tree model and assign it to the variable model.\n",
    "# You won't need to, but if you'd like, play with hyperparameters such\n",
    "# as max_depth and min_samples_leaf and see what they do to the decision\n",
    "# boundary.\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# TODO: Fit the model.\n",
    "\n",
    "model.fit(X,y)\n",
    "\n",
    "# TODO: Make predictions. Store them in the variable y_pred.\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# TODO: Calculate the accuracy and assign it to the variable acc.\n",
    "acc = accuracy_score(y,y_pred)\n",
    "\n",
    "print(\"Accuracy is : {}\".format(acc))\n",
    "\n",
    "for i in range(len(X)):\n",
    "    if y[i] == 0:\n",
    "        plt.scatter(X[i][0],X[i][1],color='b')\n",
    "        \n",
    "    else:\n",
    "        plt.scatter(X[i][0],X[i][1],color='r')\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719352f7",
   "metadata": {},
   "source": [
    "## Case Study on Titanic Survival Model with Decision Trees:\n",
    "\n",
    "The case study on titanic survival model with decision trees have been done in a seprate notebook. This can be found on my github profile. The main goal here was to bring the test accuracy above 85%. This was acheieved by hyperparameters of Decision tree model. The accuracy achieved is 86.03%.\n",
    "\n",
    "\n",
    "A snippet of last solution that was implemented to find the best hyperparamters values is shown below:\n",
    "\n",
    "![titanic](titanic.png)\n",
    "\n",
    "\n",
    "# Resources:\n",
    "---\n",
    "Udacity.com. 2022. Machine Learning with TensorFlow | Intro to TensorFlow. [online] Available at: <https://www.udacity.com/course/intro-to-machine-learning-with-tensorflow-nanodegree--nd230> [Accessed 28 January 2022].\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb05f6af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3373286",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
